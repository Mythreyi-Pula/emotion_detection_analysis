import pandas as pd


df=pd.read_csv("data.csv")

df.head(10)


df.shape

#adding a new column Emotion_num which gives unidue number to these emotions
df["Emotion_num"]=df.Emotion.map({
    'joy':0,
    'fear':1,
    'anger':2
})
df.head(10)

Modellind without preprocessing text data


from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(df.Comment,df.Emotion_num,test_size=0.2,random_state=2022,stratify=df.Emotion_num)

x_train.shape

x_test.shape

Using CountVectorizer with only trigrams and RandomForest as the classifier.

from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier

from sklearn.feature_extraction.text import CountVectorizer
clf=Pipeline([
    ('vectorizer_bow',CountVectorizer(ngram_range=(3,3))),
    ('classifier',RandomForestClassifier())
])

clf.fit(x_train,y_train)

y_pred=clf.predict(x_test)
print(classification_report(y_test,y_pred))

Using CountVectorizer with both unigram and bigrams and  Multinomial Naive Bayes as the classifier

from sklearn.naive_bayes import MultinomialNB
clf=Pipeline([
    ('vectorizer_bow',CountVectorizer(ngram_range=(1,2))),
    ('nb',MultinomialNB())
])

clf.fit(x_train,y_train)

y_pred=clf.predict(x_test)
print(classification_report(y_test,y_pred))

Using CountVectorizer with both unigram and Bigrams and RandomForest as the classifier.

clf=Pipeline([
    ('vectorizer_bow',CountVectorizer(ngram_range=(1,2))),
    ('classifier',RandomForestClassifier())
])

clf.fit(x_train,y_train)

y_pred=clf.predict(x_test)
print(classification_report(y_test,y_pred))

Using TF-IDF vectorizer for Pre-processing the text and RandomForest as the classifier.

from sklearn.feature_extraction.text import TfidfVectorizer
clf=Pipeline([
    ('vectorizer_tfidf',TfidfVectorizer()),
    ('classifier',RandomForestClassifier())
])

clf.fit(x_train,y_train)

y_pred=clf.predict(x_test)
print(classification_report(y_test,y_pred))

Using text pre-processing to remove stop words, punctuations and apply lemmatization

import spacy

# load english language model and create nlp object from it
nlp = spacy.load("en_core_web_sm") 


#use this utility function to get the preprocessed text data
def preprocess(text):
    # remove stop words and lemmatize the text
    doc = nlp(text)
    filtered_tokens = []
    for token in doc:
        if token.is_stop or token.is_punct:
            continue
        filtered_tokens.append(token.lemma_)
    
    return " ".join(filtered_tokens) 

df['preprocessed_text']=df['Comment'].apply(preprocess)

df.head(10)

Building a model with pre processed text

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(df.preprocessed_text,df.Emotion_num,test_size=0.2,random_state=2022,stratify=df.Emotion_num)

using CountVectorizer with both unigrams and bigrams and RandomForest as the classifier

clf=Pipeline([
    ('vectorizer_bow',CountVectorizer(ngram_range=(1,2))),
    ('classifier',RandomForestClassifier())
])

clf.fit(x_train,y_train)

y_pred=clf.predict(x_test)
print(classification_report(y_test,y_pred))

Using TF-IDF vectorizer for pre-processing the text and RandomForest as the classifier.

clf=Pipeline([
    ('vectorizer_tfidf',TfidfVectorizer()),
    ('classifier',RandomForestClassifier())
])

clf.fit(x_train,y_train)

y_pred=clf.predict(x_test)
print(classification_report(y_test,y_pred))

**Key Findings:**

* As the n_gram range keeps increasing, there's drastic fall of improvement in performance metrics.

* There's seen a significant improvement in results before pre-processing and after pre-processing the data.

* TF-IDF and Bag of words both performed equally well in performance metrics like Recall and F1-score.

* Random Forest performed quite well when compared to Multinomial Naive Bayes.


